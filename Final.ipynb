{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "#np.seterr(divide='print', invalid='print')\n",
    "from xgboost import XGBClassifier\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "import seaborn as sns\n",
    "import IPython\n",
    "from plotly.offline import init_notebook_mode\n",
    "%matplotlib inline\n",
    "import plotly.offline as offline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "\n",
    "from common_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"y_encoder\", \"rb\") as fp:   \n",
    "    y_encoder = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OvrXgb:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ovr_models = dict()\n",
    "        self.weight = dict()\n",
    "        for i in range(10):\n",
    "            self.weight[i] = 1\n",
    "            \n",
    "    \"\"\"\n",
    "        set new weights\n",
    "    \"\"\"\n",
    "    def set_weight(self, new_weight):\n",
    "        self.weight = new_weight\n",
    "    \n",
    "    \"\"\"\n",
    "        train model and validate\n",
    "    \"\"\"\n",
    "    def fit(self, train_x, train_y, val_x, val_y, balance=False):\n",
    "        ovr_classes = np.unique(train_y)\n",
    "        print('Training...')\n",
    "        for ovr_class in ovr_classes:\n",
    "            self.__fit_class(ovr_class, train_x, train_y, val_x, val_y, balance)\n",
    "\n",
    "\n",
    "    def __fit_class(self, ovr_class, train_x, train_y, val_x, val_y, balance):\n",
    "        #train_y_ovr = np.array([1 if y == ovr_class else 0 for y in train_y])\n",
    "        train_y_ovr = []\n",
    "        val_y_ovr = np.array([1 if y == ovr_class else 0 for y in val_y])\n",
    "\n",
    "        #classes of value 1 will be quite less since all other classes will get covered under 0. this might create imbalanced training set\n",
    "        #generating a balanced training and val subset\n",
    "        train_mask = []\n",
    "        val_mask = []\n",
    "\n",
    "        ovr_class_count = 0\n",
    "        non_ovr_class_count = 0\n",
    "        for index, y in enumerate(train_y):\n",
    "            if y == ovr_class:   #if current class then add directly\n",
    "                train_mask.append(index)\n",
    "                train_y_ovr.append(1)\n",
    "                ovr_class_count+=1\n",
    "            elif not balance: #if not balance then add directly else add only if balanced\n",
    "                train_mask.append(index)\n",
    "                train_y_ovr.append(0)\n",
    "                non_ovr_class_count+=1\n",
    "            elif ovr_class_count >= non_ovr_class_count:\n",
    "                train_mask.append(index)\n",
    "                train_y_ovr.append(0)\n",
    "                non_ovr_class_count+=1\n",
    "        \n",
    "        x_cfl=XGBClassifier(nthread=4)\n",
    "        x_cfl.fit(train_x[train_mask], train_y_ovr)\n",
    "\n",
    "        model = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
    "        model.fit(train_x[train_mask], train_y_ovr)\n",
    "        \n",
    "        self.ovr_models[ovr_class] = model #store OVR model\n",
    "\n",
    "        train_ovr_detection_acc = self.__get_acc(train_y_ovr, [int(np.round(x)) for x in model.predict(train_x[train_mask])])\n",
    "        val_ovr_detection_acc = self.__get_acc(val_y_ovr, [int(np.round(x)) for x in model.predict(val_x)])\n",
    "\n",
    "        print('Accuracy for class \"' + str(y_encoder.classes_[ovr_class]) + '\" - Train acc.: ' + str(train_ovr_detection_acc) + ' %, Val acc.: ' + str(val_ovr_detection_acc) + ' %')\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "        get accuracy\n",
    "    \"\"\"\n",
    "    def __get_acc(self, true_y, pred_y):\n",
    "        return round(accuracy_score(true_y, pred_y) * 100, 2)\n",
    "\n",
    "    \"\"\"\n",
    "        get auc score\n",
    "    \"\"\"\n",
    "    def __get_auc(self, true_y, pred_y):\n",
    "        return round(roc_auc_score(true_y, pred_y) * 100, 2)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "        predict test X\n",
    "    \"\"\"\n",
    "    def predict(self, test_x):\n",
    "        ovr_pred = np.array([])  #for storing predictions\n",
    "        ovr_proba = np.array([]) #for storing probabilities\n",
    "        for ovr_class in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "            \n",
    "            if ovr_class==0:\n",
    "                ovr_pred, ovr_proba = self.__predict_using_ovr(ovr_class, test_x)\n",
    "                ovr_proba = ovr_proba * self.weight[ovr_class]\n",
    "            else:\n",
    "                pred, proba = self.__predict_using_ovr(ovr_class, test_x)\n",
    "                proba = proba * self.weight[ovr_class]\n",
    "\n",
    "                ovr_pred = np.hstack((ovr_pred, pred))\n",
    "                ovr_proba = np.hstack((ovr_proba, proba))\n",
    "\n",
    "        result = []\n",
    "        for index, pred_row in enumerate(ovr_pred):\n",
    "            #if single prediction is present across all predictions go for this value otherwise use probabilites\n",
    "            if np.bincount(pred_row.astype(int), minlength=2)[1] == 1:\n",
    "                result.append(np.argmax(ovr_pred[index]))\n",
    "            else:\n",
    "                result.append(np.argmax(ovr_proba[index]))\n",
    "\n",
    "        return np.array(result)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "        predict probability\n",
    "    \"\"\"\n",
    "    def predict_proba(self, test_x):\n",
    "        ovr_pred = np.array([])  #for storing predictions\n",
    "        ovr_proba = np.array([]) #for storing probabilities\n",
    "        for ovr_class in np.unique([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]):\n",
    "            \n",
    "            if ovr_class==0:\n",
    "                ovr_pred, ovr_proba = self.__predict_using_ovr(ovr_class, test_x)\n",
    "                ovr_proba = ovr_proba * self.weight[ovr_class]\n",
    "            else:\n",
    "                pred, proba = self.__predict_using_ovr(ovr_class, test_x)\n",
    "                proba = proba * self.weight[ovr_class]\n",
    "\n",
    "                ovr_pred = np.hstack((ovr_pred, pred))\n",
    "                ovr_proba = np.hstack((ovr_proba, proba))\n",
    "\n",
    "        return ovr_proba\n",
    "\n",
    "    \"\"\"\n",
    "        utility method\n",
    "    \"\"\"\n",
    "    def __predict_using_ovr(self, ovr_class, test_x):\n",
    "        \n",
    "        ovr_model = self.ovr_models[ovr_class]\n",
    "\n",
    "        \"\"\"\n",
    "            use predict\n",
    "            if it fails then use proba\n",
    "        \"\"\"\n",
    "        pred = ovr_model.predict(test_x)\n",
    "        #proba = ovr_model.predict_proba(test_x) #use for keras models\n",
    "        proba = ovr_model.predict_proba(test_x)[:, 1] #use for xgb\n",
    "        return pred.reshape(-1, 1), proba.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Takes X and predicts y\n",
    "\"\"\"\n",
    "def run_pipeline_1(X):\n",
    "    \n",
    "    X = pd.DataFrame(X, columns = [\"id\", \"dur\", \"proto\", \"service\", \"state\", \"spkts\", \"dpkts\", \"sbytes\", \"dbytes\", \"rate\", \"sttl\", \"dttl\", \"sload\", \n",
    "                                \"dload\", \"sloss\", \"dloss\", \"sinpkt\", \"dinpkt\", \"sjit\", \"djit\", \"swin\", \"stcpb\", \"dtcpb\", \"dwin\", \"tcprtt\", \"synack\", \n",
    "                                \"ackdat\", \"smean\", \"dmean\", \"trans_depth\", \"response_body_len\", \"ct_srv_src\", \"ct_state_ttl\", \"ct_dst_ltm\", \n",
    "                                \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\", \"ct_dst_src_ltm\", \"is_ftp_login\", \"ct_ftp_cmd\", \"ct_flw_http_mthd\", \"ct_src_ltm\", \n",
    "                                \"ct_srv_dst\", \"is_sm_ips_ports\"])\n",
    "    \n",
    "    X = X.drop(['id'], axis=1)\n",
    "    \n",
    "    #2. Encode\n",
    "    with open(\"binary_features\", \"rb\") as fp:   # Unpickling binary features\n",
    "        binary_features = pickle.load(fp)\n",
    "\n",
    "    with open(\"categorical_features\", \"rb\") as fp:   # Unpickling categorical features\n",
    "        categorical_features = pickle.load(fp)\n",
    "\n",
    "    with open(\"numerical_features\", \"rb\") as fp:   # Unpickling numerical features\n",
    "        numerical_features = pickle.load(fp)\n",
    "    \n",
    "    with open(\"one_hot_encoders\", \"rb\") as fp:   \n",
    "        one_hot_encoders = pickle.load(fp)\n",
    "        \n",
    "    with open(\"one_hot_encoders_features\", \"rb\") as fp:   \n",
    "        one_hot_encoders_features = pickle.load(fp)\n",
    "        \n",
    "    with open(\"feature_standardizers\", \"rb\") as fp: \n",
    "        feature_standardizers = pickle.load(fp)\n",
    "    \n",
    "    with open(\"minmax_scalers\", \"rb\") as fp: \n",
    "        minmax_scalers = pickle.load(fp)\n",
    "        \n",
    "    X_encoded, _ = merge_all(X, one_hot_encoders_features, \n",
    "                                      get_one_hot_encoded_features(X, categorical_features, one_hot_encoders), \n",
    "                                      get_standardized_features(X, numerical_features, feature_standardizers), \n",
    "                                      binary_features)\n",
    "\n",
    "    with open(\"random_forest\", \"rb\") as fp: \n",
    "        random_forest = pickle.load(fp)  \n",
    "    \n",
    "    y_1_probas = random_forest.predict_proba(X_encoded)\n",
    "    # print(y_1.shape)\n",
    "    #y = np.array([np.argmax(y) for y in y_1]).reshape(-1, 1)\n",
    "    \n",
    "    y_1_pred = []\n",
    "    threshold = 0.980280 # best threshold from previous notebook\n",
    "    for y_proba in y_1_probas[:, 1]: \n",
    "        if y_proba < threshold:\n",
    "            y_1_pred.append(0)\n",
    "        else:\n",
    "            y_1_pred.append(1)\n",
    "            \n",
    "    y_1_pred = np.array(y_1_pred).reshape(-1, 1)\n",
    "    \n",
    "    #print(y.shape)\n",
    "    #print(X_encoded.shape)\n",
    "    X_encoded = np.hstack((X_encoded.todense(), y_1_pred))\n",
    "        \n",
    "    oxgb = OvrXgb()\n",
    "    with open(\"oxgb_models\", \"rb\") as fp:   # Unpickling model\n",
    "        oxgb.ovr_models = pickle.load(fp)\n",
    "    \n",
    "    y_proba = oxgb.predict_proba(X_encoded)\n",
    "    \n",
    "    y_pred = np.array([np.argmax(y) for y in y_proba]).reshape(-1, 1)\n",
    "    #print(\"Predicted class is \" + y_encoder.inverse_transform(y_pred.ravel()))\n",
    "    \n",
    "    return y_pred, y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_encoded shape: (1, 196), features_encoded_name len: 196\n",
      "['Predicted class is Generic']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "117295,0.000006,udp,dns,INT,2,0,114,0,166666.6608,254,0,76000000,0,0,0,0.006,0,0,0,0,0,0,0,0,0,0,57,0,0,0,32,2,16,16,16,32,0,0,0,16,32,0,\n",
    "Generic,1\n",
    "\"\"\"\n",
    "\n",
    "y_pred, y_proba = run_pipeline_1([[1, 0.000006,\"udp\",\"dns\",\"INT\",2,0,114,0,166666.6608,254,0,76000000,0,0,0,0.006,0,0,0,0,0,0,0,0,0,0,57,0,0,0,32,2,16,16,16,32,0,0,0,16,32,0]])\n",
    "\n",
    "print(\"Predicted class is \" + y_encoder.inverse_transform(y_pred.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_encoded shape: (1, 196), features_encoded_name len: 196\n",
      "['Predicted class is Normal']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "44091,1.504671,tcp,http,FIN,10,10,878,2606,12.627345,62,252,4205.570801,12473.15918,2,2,167.185667,157.208562,\n",
    "10025.67205,257.018641,255,2630345556,2391717210,255,0.168907,0.088862,0.080045,88,261,0,0,2,1,1,1,1,1,0,0,0,2,2,0,\n",
    "Exploits,1\n",
    "\"\"\"\n",
    "\n",
    "y_pred, y_proba = run_pipeline_1([[44091,1.504671,\"tcp\",\"http\",\"FIN\",10,10,878,2606,12.627345,62,252,4205.570801,12473.15918,2,2,167.185667,157.208562,10025.67205,257.018641,255,2630345556,2391717210,255,0.168907,0.088862,0.080045,88,261,0,0,2,1,1,1,1,1,0,0,0,2,2,0]])\n",
    "\n",
    "print(\"Predicted class is \" + y_encoder.inverse_transform(y_pred.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/UNSW_NB15_testing-set.csv\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Takes X, y and runs model to get y_pred and displays metrics\n",
    "\"\"\"\n",
    "def run_pipeline_2(X, y):\n",
    "    y_pred, y_proba = run_pipeline_1(X)\n",
    "\n",
    "    y_true = y_encoder.transform(y)\n",
    "\n",
    "    print (\"log loss for data : \" + str(log_loss(y_true, y_proba)))\n",
    "    print(\"accuracy score is \" + str(accuracy_score(y_true, y_pred)))\n",
    "    \n",
    "    return y_pred, y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_encoded shape: (82332, 196), features_encoded_name len: 196\n",
      "log loss for data : 1.0040835032678204\n",
      "accuracy score is 0.7515182432104164\n"
     ]
    }
   ],
   "source": [
    "y_pred, y_proba = run_pipeline_2(test.drop([\"label\", \"attack_cat\"], axis=1), test.attack_cat.to_numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
